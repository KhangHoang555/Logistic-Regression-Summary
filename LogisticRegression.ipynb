{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5356837-f723-4f07-98aa-388d0b34ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will attempt to create a Regression model predicting 'verified_status'\n",
    "# Y(Dependent) = 'verified_status'\n",
    "# Potential X(Independent) = \"video_duration_sec\", \"claim_status\", \"author_ban_status\", \"video_view_count\", \n",
    "#                    \"video_like_count\", \"video_share_count\", \"video_download_count\", \"video_comment_count\", \"video_transcription_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d520f-9ca7-49a4-9596-115b0f872246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import packages for data preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Import package to download data\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5a229-7e47-4452-86f0-b9590adf4fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into dataframe\n",
    "data = pd.read_csv(\"tiktok_dataset.csv\")\n",
    "file_path = 'tiktok_dataset.csv'\n",
    "FileLink(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d93a7a-b632-47a7-bdeb-9f391063f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "data.describe()\n",
    "data.shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9e86c-7752-4d55-9520-530843219f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and duplicated then drop missing values\n",
    "data.isna().sum()\n",
    "data.duplicated().sum()\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e7bf9-f185-4b3c-84d2-7312b173c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for EDA, visualizing distribution of CONTINUOUS X(independent) variables\n",
    "# x = \"video_duration_sec\", \"video_view_count\", video_like_count\", \"video_comment_count\"\n",
    "# We will ignore \"video_id\", this was assigned by our system.\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.title('Boxplot to detect outliers for video_duration_sec', fontsize=12)\n",
    "sns.boxplot(x=data['video_duration_sec'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4e758-691a-4d39-aa11-46bdd260670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since outliers can strongly influence our model\n",
    "# we will control outliers by reducing any extreme outlier \n",
    "# down into the quantile UPPER limit(quant3 + 1.5 *iqr)\n",
    "# We will do this for both \"VIDEO_LIKE_COUNT\" and \"VIDEO_COMMENT_COUNT\"\n",
    "# since they have a lot of outrageous outliers based on the boxplot visualization\n",
    "\n",
    "\n",
    "quant1 = data['video_like_count'].quantile(0.25)\n",
    "quant3 = data['video_like_count'].quantile(0.75)\n",
    "iqr = quant3 - quant1                            # Find IQR\n",
    "upper_lim = quant3 + 1.5 *iqr                    # Find Upper Limit\n",
    "\n",
    "# Assigning upper_limit into any value that is larger than upper limit\n",
    "data.loc[data[\"video_like_count\"] > upper_lim, \"video_like_count\"] = upper_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e6bfd-5781-4ba6-a9a7-955422f6cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will EDA and data clean our CATEGORICAL X Variables\n",
    "data['verified_status'].value_counts(normalize=True) # normalize=True turn counting into percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad1820-3607-4167-9cd6-e6beb4feae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_status\n",
    "not verified    0.93712\n",
    "verified        0.06288\n",
    "Name: proportion, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe66fc-0a3c-469e-add7-c646f8d0d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \"verified\" is SIGNIFICANTLY lower than 'not verified', we need to \n",
    "# balance it out using resample()\n",
    "\n",
    "\n",
    "# Split 'verified_status' data into major and minor classes\n",
    "data_major = data[data['verified_status'] == 'not verified']\n",
    "data_minor = data[data['verified_status'] == 'verified']\n",
    "\n",
    "# Upsampling minor 'verified' to be equal with major 'not verified'\n",
    "data_minor_upsampled = resample(data_minor,\n",
    "                                replace = True,# True = replicating original data more than ONCE is OK\n",
    "                                n_samples = len(data_major),\n",
    "                                random_state = 0)\n",
    "\n",
    "# Put the splitted data back together with the newly upsampled data\n",
    "data_upsampled = pd.concat([data_major,data_minor_upsampled]).reset_index(drop = True)\n",
    "# Recheck if it was done correctly\n",
    "data_upsampled['verified_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bcf6604-4f62-4471-9a22-9a53175879d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_upsampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m array])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m status_mean \u001b[38;5;241m=\u001b[39m \u001b[43mdata_upsampled\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverified_status\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_transcription_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverified_status\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg((count_text))\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      8\u001b[0m status_mean\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_upsampled' is not defined"
     ]
    }
   ],
   "source": [
    "# We will try to quantify `video_transcription_text` by measuring the length of the text\n",
    "data_upsampled['text_length'] = data_upsampled['video_transcription_text'].apply(lambda text: len(text))\n",
    "\n",
    "# Comparing 2 categories in 'verified_status' for any significant difference\n",
    "data_upsampled.groupby('verified_status')['text_length'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6c1f0-37ca-42b7-9346-e63fede428c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of `video_transcription_text` length for videos\n",
    "# posted by verified accounts and videos posted by unverified accounts\n",
    "sns.histplot(data = data_upsampled,\n",
    "             stat = 'count',\n",
    "             multiple = 'stack',\n",
    "             x = 'text_length',\n",
    "             kde = False,\n",
    "             hue = 'verified_status',\n",
    "             element = 'bars', legend = True\n",
    "            )\n",
    "\n",
    "# Extra details\n",
    "plt.xlabel(\"video_transcription_text length (number of characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of video_transcription_text length for videos posted by verified accounts and videos posted by unverified accounts\")\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6577ae-d11b-499e-b19a-bdf179df915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code a correlation matrix to help determine most correlated variables\n",
    "data_upsampled.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940abc04-fe43-4e4b-8c07-29fcb1fecf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlated variables\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(\n",
    "    data_upsampled[[\"video_duration_sec\", \"claim_status\", \"author_ban_status\", \"video_view_count\", \n",
    "                    \"video_like_count\", \"video_share_count\", \"video_download_count\", \"video_comment_count\", \"text_length\"]]\n",
    "    .corr(numeric_only=True), \n",
    "    annot=True, \n",
    "    cmap=\"crest\") #color palette\n",
    "\n",
    "plt.title(\"Heatmap of the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a6c58-4e67-4982-962f-4d6cfd162ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have all variables set up properly. We finally can create our Logistic Model\n",
    "# predicting Y using X as stated in beginning\n",
    "# Separate up X and Y for the Model\n",
    "y = data_upsampled['verified_status']     \n",
    "\n",
    "\n",
    "\n",
    "X = data_upsampled[[\"video_duration_sec\", \"claim_status\", \"author_ban_status\", \"video_view_count\", \n",
    "                    \"video_share_count\", \"video_download_count\", \"video_comment_count\"]]\n",
    "# We do not to include text_length because it was stastistically insignificant\n",
    "# according to heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefc0d9-992a-4824-b7d2-3dd3719b9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data FURTHER into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "# Check the splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927073e-4f80-48a0-8fb8-6fd1dce62600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of categorical values `claim_status` and `author_ban_status` \n",
    "# to see what they look like\n",
    "X_train[\"claim_status\"].unique()           # array(['opinion', 'claim'], dtype=object)\n",
    "X_train[\"author_ban_status\"].unique()      # array(['active', 'under review', 'banned'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcae178-bce0-4d42-8d84-38fc5e6d8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP X TRAIN ##\n",
    "\n",
    "# Converting categorical values into numeric so model can calculate them\n",
    "# Taking category \"claim_status\", \"author_ban_status\" together \n",
    "# for efficient processing and removing them from the dataframe\n",
    "X_train_to_encode = X_train[[\"claim_status\", \"author_ban_status\"]]\n",
    "X_train = X_train.drop(columns=[\"claim_status\", \"author_ban_status\"])\n",
    "\n",
    "\n",
    "# Using One Hot Encoding technique to encode categorical data\n",
    "X_encoder = OneHotEncoder(drop='first', sparse_output=False) # Formula\n",
    "X_train_encoded = X_encoder.fit_transform(X_train_to_encode) # Fitting data into formula\n",
    "\n",
    "\n",
    "# Since the array generated after fit_transform is not useable by our model because they are missing column names. We need to \n",
    "# convert it back into a Dataframe and give them names.\n",
    "X_train_encoded_df = pd.DataFrame(data=X_train_encoded, columns=X_encoder.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Concatenate `X_train` and `X_train_encoded_df` to form the final dataframe for training data (`X_train_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_train after dropping `claim_status` and `author_ban_status`,\n",
    "# so that the indices align with those in `X_train_encoded_df` and `count_df`\n",
    "\n",
    "X_train_final = pd.concat([X_train.reset_index(drop = True),X_train_encoded_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7825820-72d0-4dc8-a7a7-7891b0b0efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP X TEST ##\n",
    "# While we are at it, setting up X_test right after X_train for better organization\n",
    "X_test_to_encode = X_test[['claim_status', 'author_ban_status']]\n",
    "X_test = X_test.drop(columns=[\"claim_status\", \"author_ban_status\"])\n",
    "\n",
    "#Reuse Encoder since they both use the same formula\n",
    "X_test_encoded = X_encoder.transform(X_test_to_encode)\n",
    "\n",
    "X_test_encoded_df = pd.DataFrame(data = X_test_encoded, columns = X_encoder.get_feature_names_out())\n",
    "\n",
    "X_test_final = pd.concat([X_test.reset_index(drop = True), X_test_encoded_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d0569-a7dc-453d-8521-3a244d1712af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTING UP Y TRAIN ##\n",
    "\n",
    "# Get unique values of outcome variable\n",
    "y_train.unique() #array(['verified', 'not verified'], dtype=object)\n",
    "\n",
    "# Setting up formula for conversion\n",
    "y_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Adjusting the shape of `y_train` before passing into `.fit_transform()`,\n",
    "# since it takes in 2D array\n",
    "# .Ravel() at the end to flatten back to 1D array so it retains original form\n",
    "y_train_final = y_encoder.fit_transform(y_train.values.reshape(-1,1),).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c8f31-c8e4-497d-8756-8b4a7a0f5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTING UP Y TEST ##\n",
    "\n",
    "#Reuse Encoder since they both use the same formula\n",
    "y_test_final = y_encoder.fit_transform(y_test.values.reshape(-1,1),).ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881784c-e0ce-4f30-a444-0c8498e28d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconfirm sizes of all train and tests data\n",
    "X_train_final.shape, y_train_final.shape, X_test_final.shape, y_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93664173-e080-4fe9-9e56-dcc0e951704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "# CONSTRUCT the logistic regression model using TRAINING data to get PREDICTIONs\n",
    "# then\n",
    "# EVALUATE by COMPARE PREDICTIONs against TEST data for accuracy\n",
    "\n",
    "log_clf = LogisticRegression(random_state = 0, max_iter = 800).fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832330ca-af3b-4cf1-bd14-d76f60d67de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "# Storing predictions\n",
    "y_pred = log_clf.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13fc83-5065-4e73-9b8b-7122dbe781b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION Visually\n",
    "\n",
    "log_cm = confusion_matrix(y_test_final, y_pred, labels = log_clf.classes_) # Send in data\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix = log_cm, display_labels = log_clf.classes_) # Selecting Visual Details\n",
    "\n",
    "log_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5abe37-4d94-4b52-a31d-800349855c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION Statistically\n",
    "accuracy = (3758+2044) / (3758 + 725 + 2044 + 2415)\n",
    "accuracy\n",
    "\n",
    "#Getting Accuracy, Precision and Recall to evaluate\n",
    "target_labels = [\"verified\", \"not verified\"]\n",
    "print(classification_report(y_test_final, y_pred, target_names=target_labels))\n",
    "\n",
    "log_clf.intercept_\n",
    "\n",
    "# Getting coefficients of all X values in predict Y('verified_status')\n",
    "pd.DataFrame(data={\"Feature Name\":log_clf.feature_names_in_, \"Model Coefficient\":log_clf.coef_[0]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
